{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "benchmark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+bxBo6g9XF3Wzq/j3pkP9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakepon21/Masa/blob/Signate2/benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsOjhujEmJrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ライブラリのインポート\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "warnings.simplefilter('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvcmmu0vmSZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a56dd4d1-c453-48a5-ed04-e26cb5dcdef7"
      },
      "source": [
        "#train,test,submit_sampleのみっつがそろっているか確認\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjRXtP4amTOj",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCkgewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwogICAgICBwZXJjZW50LnRleHRDb250ZW50ID0KICAgICAgICAgIGAke01hdGgucm91bmQoKHBvc2l0aW9uIC8gZmlsZURhdGEuYnl0ZUxlbmd0aCkgKiAxMDApfSUgZG9uZWA7CiAgICB9CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "64185a26-02e6-4f55-abac-0160a161d37a"
      },
      "source": [
        "#そろっていなかったら選択\n",
        "from google.colab import files\n",
        "train_up = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-07d35e57-58b2-40f5-bd0d-ae9179a3f0ce\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-07d35e57-58b2-40f5-bd0d-ae9179a3f0ce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sample_submit.csv to sample_submit.csv\n",
            "Saving test.csv to test.csv\n",
            "Saving train.csv to train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLmG8tOKIj-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "88318deb-b052-443a-ce1a-a6a13721e8ad"
      },
      "source": [
        "submit_df = pd.read_csv(\"sample_submit.csv\",names=(\"A\",\"B\"))\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# データの量の確認\n",
        "train_df.shape,test_df.shape,submit_df.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((850, 11), (350, 10), (350, 2))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI4JnsGlIukA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 訓練データ、テストデータがわかるようにダミーの目的変数を代入\n",
        "test_df['disease']=-999"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf87rqRNIw7-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "49330106-9e02-4c5a-a737-80d776c760c5"
      },
      "source": [
        "# 訓練データ、テストデータを結合\n",
        "all_df = pd.concat([train_df,test_df])\n",
        "del train_df,test_df\n",
        "gc.collect()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "647"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOJEuPp7IyLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# カテゴリカラムの前処理\n",
        "categorical_features = [\"Gender\"]\n",
        "for col in categorical_features:\n",
        "    lbl = preprocessing.LabelEncoder()\n",
        "    lbl.fit(all_df[col])\n",
        "    lbl.transform(all_df[col])\n",
        "    all_df[col]=lbl.transform(all_df[col])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHzdi5GUIzUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 訓練データ、テストデータの分割\n",
        "train_df = all_df[all_df['disease']!=-999]\n",
        "test_df = all_df[all_df['disease']==-999]"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SnXLNhYI0fd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_origin = train_df['disease']\n",
        "X_train_origin = train_df.drop(['disease'], axis=1)\n",
        "X_test_origin = test_df.drop(['disease'], axis=1)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trCS4KG0MleJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c8e0986-e055-45e0-8849-5ec42b3d4bf1"
      },
      "source": [
        "output_df = pd.DataFrame()\n",
        "for i in range(100):\n",
        "    #訓練データからテストデータを分割\n",
        "    X_train , X_valid , y_train , y_valid = train_test_split(X_train_origin,y_train_origin,test_size = 0.3 , random_state = i)\n",
        "    #使用モデルはLGB（パラメーターチューニングなし）\n",
        "    lgb_train = lgb.Dataset(X_train,y_train,categorical_feature = categorical_features)\n",
        "    lgb_eval = lgb.Dataset(X_valid , y_valid , reference = lgb_train , categorical_feature = categorical_features)\n",
        "    params = {\n",
        "        \"objective\":\"binary\"\n",
        "    }\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,lgb_train,\n",
        "        valid_sets=[lgb_train,lgb_eval],\n",
        "        verbose_eval = 10,\n",
        "        num_boost_round = 1000,\n",
        "        early_stopping_rounds=10\n",
        "    )\n",
        "\n",
        "    y_pred = model.predict(X_test_origin,num_iteration=model.best_iteration)\n",
        "    output_df[i] = y_pred\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.356897\tvalid_1's binary_logloss: 0.386781\n",
            "[20]\ttraining's binary_logloss: 0.241681\tvalid_1's binary_logloss: 0.299914\n",
            "[30]\ttraining's binary_logloss: 0.171527\tvalid_1's binary_logloss: 0.261943\n",
            "[40]\ttraining's binary_logloss: 0.125409\tvalid_1's binary_logloss: 0.244551\n",
            "[50]\ttraining's binary_logloss: 0.0914845\tvalid_1's binary_logloss: 0.23815\n",
            "[60]\ttraining's binary_logloss: 0.0662456\tvalid_1's binary_logloss: 0.237305\n",
            "Early stopping, best iteration is:\n",
            "[59]\ttraining's binary_logloss: 0.0685872\tvalid_1's binary_logloss: 0.235164\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.349835\tvalid_1's binary_logloss: 0.413977\n",
            "[20]\ttraining's binary_logloss: 0.226958\tvalid_1's binary_logloss: 0.324278\n",
            "[30]\ttraining's binary_logloss: 0.162315\tvalid_1's binary_logloss: 0.307425\n",
            "[40]\ttraining's binary_logloss: 0.11866\tvalid_1's binary_logloss: 0.299668\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.130229\tvalid_1's binary_logloss: 0.299284\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.35705\tvalid_1's binary_logloss: 0.389238\n",
            "[20]\ttraining's binary_logloss: 0.23141\tvalid_1's binary_logloss: 0.304831\n",
            "[30]\ttraining's binary_logloss: 0.163965\tvalid_1's binary_logloss: 0.267913\n",
            "[40]\ttraining's binary_logloss: 0.116123\tvalid_1's binary_logloss: 0.258154\n",
            "[50]\ttraining's binary_logloss: 0.0831244\tvalid_1's binary_logloss: 0.255309\n",
            "Early stopping, best iteration is:\n",
            "[49]\ttraining's binary_logloss: 0.0862382\tvalid_1's binary_logloss: 0.254036\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352821\tvalid_1's binary_logloss: 0.41245\n",
            "[20]\ttraining's binary_logloss: 0.226662\tvalid_1's binary_logloss: 0.341044\n",
            "[30]\ttraining's binary_logloss: 0.159629\tvalid_1's binary_logloss: 0.318545\n",
            "[40]\ttraining's binary_logloss: 0.114248\tvalid_1's binary_logloss: 0.316713\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.130589\tvalid_1's binary_logloss: 0.313435\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342867\tvalid_1's binary_logloss: 0.427652\n",
            "[20]\ttraining's binary_logloss: 0.216514\tvalid_1's binary_logloss: 0.370545\n",
            "[30]\ttraining's binary_logloss: 0.146424\tvalid_1's binary_logloss: 0.355566\n",
            "[40]\ttraining's binary_logloss: 0.103319\tvalid_1's binary_logloss: 0.347207\n",
            "[50]\ttraining's binary_logloss: 0.0724778\tvalid_1's binary_logloss: 0.356341\n",
            "Early stopping, best iteration is:\n",
            "[42]\ttraining's binary_logloss: 0.0963736\tvalid_1's binary_logloss: 0.343915\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.343057\tvalid_1's binary_logloss: 0.411224\n",
            "[20]\ttraining's binary_logloss: 0.224486\tvalid_1's binary_logloss: 0.352053\n",
            "[30]\ttraining's binary_logloss: 0.156926\tvalid_1's binary_logloss: 0.334106\n",
            "[40]\ttraining's binary_logloss: 0.111377\tvalid_1's binary_logloss: 0.339092\n",
            "Early stopping, best iteration is:\n",
            "[31]\ttraining's binary_logloss: 0.15182\tvalid_1's binary_logloss: 0.333285\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.338302\tvalid_1's binary_logloss: 0.436502\n",
            "[20]\ttraining's binary_logloss: 0.21476\tvalid_1's binary_logloss: 0.379143\n",
            "[30]\ttraining's binary_logloss: 0.148188\tvalid_1's binary_logloss: 0.356036\n",
            "[40]\ttraining's binary_logloss: 0.10677\tvalid_1's binary_logloss: 0.360032\n",
            "Early stopping, best iteration is:\n",
            "[31]\ttraining's binary_logloss: 0.143515\tvalid_1's binary_logloss: 0.353876\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.353143\tvalid_1's binary_logloss: 0.404446\n",
            "[20]\ttraining's binary_logloss: 0.237088\tvalid_1's binary_logloss: 0.339246\n",
            "[30]\ttraining's binary_logloss: 0.167837\tvalid_1's binary_logloss: 0.315397\n",
            "[40]\ttraining's binary_logloss: 0.12101\tvalid_1's binary_logloss: 0.317835\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.137597\tvalid_1's binary_logloss: 0.308896\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.335674\tvalid_1's binary_logloss: 0.424674\n",
            "[20]\ttraining's binary_logloss: 0.219126\tvalid_1's binary_logloss: 0.359726\n",
            "[30]\ttraining's binary_logloss: 0.154754\tvalid_1's binary_logloss: 0.349371\n",
            "[40]\ttraining's binary_logloss: 0.11165\tvalid_1's binary_logloss: 0.351912\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's binary_logloss: 0.129745\tvalid_1's binary_logloss: 0.346646\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.34301\tvalid_1's binary_logloss: 0.397148\n",
            "[20]\ttraining's binary_logloss: 0.223969\tvalid_1's binary_logloss: 0.320096\n",
            "[30]\ttraining's binary_logloss: 0.15914\tvalid_1's binary_logloss: 0.30408\n",
            "[40]\ttraining's binary_logloss: 0.114686\tvalid_1's binary_logloss: 0.311028\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.150384\tvalid_1's binary_logloss: 0.30203\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.355289\tvalid_1's binary_logloss: 0.398201\n",
            "[20]\ttraining's binary_logloss: 0.234935\tvalid_1's binary_logloss: 0.313104\n",
            "[30]\ttraining's binary_logloss: 0.167807\tvalid_1's binary_logloss: 0.288929\n",
            "[40]\ttraining's binary_logloss: 0.117277\tvalid_1's binary_logloss: 0.278905\n",
            "[50]\ttraining's binary_logloss: 0.0849138\tvalid_1's binary_logloss: 0.278213\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's binary_logloss: 0.096257\tvalid_1's binary_logloss: 0.275868\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.350721\tvalid_1's binary_logloss: 0.42269\n",
            "[20]\ttraining's binary_logloss: 0.224669\tvalid_1's binary_logloss: 0.344684\n",
            "[30]\ttraining's binary_logloss: 0.15828\tvalid_1's binary_logloss: 0.329768\n",
            "[40]\ttraining's binary_logloss: 0.112845\tvalid_1's binary_logloss: 0.329621\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.1164\tvalid_1's binary_logloss: 0.326689\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.348114\tvalid_1's binary_logloss: 0.403546\n",
            "[20]\ttraining's binary_logloss: 0.222666\tvalid_1's binary_logloss: 0.334568\n",
            "[30]\ttraining's binary_logloss: 0.156958\tvalid_1's binary_logloss: 0.32276\n",
            "[40]\ttraining's binary_logloss: 0.111426\tvalid_1's binary_logloss: 0.318784\n",
            "[50]\ttraining's binary_logloss: 0.0804827\tvalid_1's binary_logloss: 0.32044\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's binary_logloss: 0.0979872\tvalid_1's binary_logloss: 0.317842\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346002\tvalid_1's binary_logloss: 0.443616\n",
            "[20]\ttraining's binary_logloss: 0.218444\tvalid_1's binary_logloss: 0.384365\n",
            "[30]\ttraining's binary_logloss: 0.151567\tvalid_1's binary_logloss: 0.37832\n",
            "Early stopping, best iteration is:\n",
            "[25]\ttraining's binary_logloss: 0.180454\tvalid_1's binary_logloss: 0.374803\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.347543\tvalid_1's binary_logloss: 0.40092\n",
            "[20]\ttraining's binary_logloss: 0.223529\tvalid_1's binary_logloss: 0.333394\n",
            "[30]\ttraining's binary_logloss: 0.156155\tvalid_1's binary_logloss: 0.315695\n",
            "[40]\ttraining's binary_logloss: 0.111467\tvalid_1's binary_logloss: 0.318627\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.145818\tvalid_1's binary_logloss: 0.314739\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.347876\tvalid_1's binary_logloss: 0.424327\n",
            "[20]\ttraining's binary_logloss: 0.222292\tvalid_1's binary_logloss: 0.363856\n",
            "[30]\ttraining's binary_logloss: 0.153505\tvalid_1's binary_logloss: 0.350592\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.164989\tvalid_1's binary_logloss: 0.349639\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346284\tvalid_1's binary_logloss: 0.404127\n",
            "[20]\ttraining's binary_logloss: 0.223286\tvalid_1's binary_logloss: 0.335149\n",
            "[30]\ttraining's binary_logloss: 0.155571\tvalid_1's binary_logloss: 0.306752\n",
            "[40]\ttraining's binary_logloss: 0.111039\tvalid_1's binary_logloss: 0.309654\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.145505\tvalid_1's binary_logloss: 0.306329\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.341751\tvalid_1's binary_logloss: 0.423669\n",
            "[20]\ttraining's binary_logloss: 0.22076\tvalid_1's binary_logloss: 0.363574\n",
            "[30]\ttraining's binary_logloss: 0.157115\tvalid_1's binary_logloss: 0.349714\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.167115\tvalid_1's binary_logloss: 0.348477\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342403\tvalid_1's binary_logloss: 0.420149\n",
            "[20]\ttraining's binary_logloss: 0.22079\tvalid_1's binary_logloss: 0.360229\n",
            "[30]\ttraining's binary_logloss: 0.156502\tvalid_1's binary_logloss: 0.345006\n",
            "[40]\ttraining's binary_logloss: 0.110923\tvalid_1's binary_logloss: 0.345087\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.122317\tvalid_1's binary_logloss: 0.341109\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.349076\tvalid_1's binary_logloss: 0.421655\n",
            "[20]\ttraining's binary_logloss: 0.222537\tvalid_1's binary_logloss: 0.358143\n",
            "[30]\ttraining's binary_logloss: 0.155538\tvalid_1's binary_logloss: 0.343637\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.166916\tvalid_1's binary_logloss: 0.342588\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.354666\tvalid_1's binary_logloss: 0.400806\n",
            "[20]\ttraining's binary_logloss: 0.230454\tvalid_1's binary_logloss: 0.326525\n",
            "[30]\ttraining's binary_logloss: 0.164638\tvalid_1's binary_logloss: 0.304977\n",
            "[40]\ttraining's binary_logloss: 0.118936\tvalid_1's binary_logloss: 0.301073\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.122803\tvalid_1's binary_logloss: 0.299951\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.34653\tvalid_1's binary_logloss: 0.425471\n",
            "[20]\ttraining's binary_logloss: 0.217299\tvalid_1's binary_logloss: 0.376344\n",
            "[30]\ttraining's binary_logloss: 0.150721\tvalid_1's binary_logloss: 0.366327\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.163044\tvalid_1's binary_logloss: 0.36381\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344467\tvalid_1's binary_logloss: 0.4162\n",
            "[20]\ttraining's binary_logloss: 0.228656\tvalid_1's binary_logloss: 0.331081\n",
            "[30]\ttraining's binary_logloss: 0.161171\tvalid_1's binary_logloss: 0.304629\n",
            "[40]\ttraining's binary_logloss: 0.114582\tvalid_1's binary_logloss: 0.308641\n",
            "Early stopping, best iteration is:\n",
            "[30]\ttraining's binary_logloss: 0.161171\tvalid_1's binary_logloss: 0.304629\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.335722\tvalid_1's binary_logloss: 0.440221\n",
            "[20]\ttraining's binary_logloss: 0.214187\tvalid_1's binary_logloss: 0.385276\n",
            "[30]\ttraining's binary_logloss: 0.14868\tvalid_1's binary_logloss: 0.370425\n",
            "[40]\ttraining's binary_logloss: 0.107305\tvalid_1's binary_logloss: 0.37299\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's binary_logloss: 0.113718\tvalid_1's binary_logloss: 0.369159\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.338017\tvalid_1's binary_logloss: 0.417482\n",
            "[20]\ttraining's binary_logloss: 0.218776\tvalid_1's binary_logloss: 0.361177\n",
            "[30]\ttraining's binary_logloss: 0.155604\tvalid_1's binary_logloss: 0.352917\n",
            "[40]\ttraining's binary_logloss: 0.110891\tvalid_1's binary_logloss: 0.353541\n",
            "Early stopping, best iteration is:\n",
            "[34]\ttraining's binary_logloss: 0.134202\tvalid_1's binary_logloss: 0.349399\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344915\tvalid_1's binary_logloss: 0.431283\n",
            "[20]\ttraining's binary_logloss: 0.218893\tvalid_1's binary_logloss: 0.360236\n",
            "[30]\ttraining's binary_logloss: 0.147107\tvalid_1's binary_logloss: 0.361891\n",
            "Early stopping, best iteration is:\n",
            "[23]\ttraining's binary_logloss: 0.194599\tvalid_1's binary_logloss: 0.356157\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344827\tvalid_1's binary_logloss: 0.419931\n",
            "[20]\ttraining's binary_logloss: 0.230235\tvalid_1's binary_logloss: 0.353799\n",
            "[30]\ttraining's binary_logloss: 0.166298\tvalid_1's binary_logloss: 0.319009\n",
            "[40]\ttraining's binary_logloss: 0.119422\tvalid_1's binary_logloss: 0.307588\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.123039\tvalid_1's binary_logloss: 0.306895\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.347871\tvalid_1's binary_logloss: 0.3842\n",
            "[20]\ttraining's binary_logloss: 0.230844\tvalid_1's binary_logloss: 0.301024\n",
            "[30]\ttraining's binary_logloss: 0.163724\tvalid_1's binary_logloss: 0.278028\n",
            "[40]\ttraining's binary_logloss: 0.115577\tvalid_1's binary_logloss: 0.26852\n",
            "[50]\ttraining's binary_logloss: 0.0825187\tvalid_1's binary_logloss: 0.269234\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's binary_logloss: 0.0920596\tvalid_1's binary_logloss: 0.267648\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342453\tvalid_1's binary_logloss: 0.423086\n",
            "[20]\ttraining's binary_logloss: 0.219077\tvalid_1's binary_logloss: 0.360175\n",
            "[30]\ttraining's binary_logloss: 0.149714\tvalid_1's binary_logloss: 0.348837\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.162017\tvalid_1's binary_logloss: 0.348276\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352181\tvalid_1's binary_logloss: 0.405492\n",
            "[20]\ttraining's binary_logloss: 0.22852\tvalid_1's binary_logloss: 0.331503\n",
            "[30]\ttraining's binary_logloss: 0.159377\tvalid_1's binary_logloss: 0.308981\n",
            "[40]\ttraining's binary_logloss: 0.112976\tvalid_1's binary_logloss: 0.305984\n",
            "Early stopping, best iteration is:\n",
            "[34]\ttraining's binary_logloss: 0.137883\tvalid_1's binary_logloss: 0.304755\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.341709\tvalid_1's binary_logloss: 0.430384\n",
            "[20]\ttraining's binary_logloss: 0.221133\tvalid_1's binary_logloss: 0.375832\n",
            "[30]\ttraining's binary_logloss: 0.156025\tvalid_1's binary_logloss: 0.343025\n",
            "[40]\ttraining's binary_logloss: 0.11094\tvalid_1's binary_logloss: 0.342116\n",
            "[50]\ttraining's binary_logloss: 0.0793718\tvalid_1's binary_logloss: 0.348723\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's binary_logloss: 0.108069\tvalid_1's binary_logloss: 0.340125\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346817\tvalid_1's binary_logloss: 0.406148\n",
            "[20]\ttraining's binary_logloss: 0.225687\tvalid_1's binary_logloss: 0.334374\n",
            "[30]\ttraining's binary_logloss: 0.159383\tvalid_1's binary_logloss: 0.30956\n",
            "[40]\ttraining's binary_logloss: 0.11492\tvalid_1's binary_logloss: 0.306746\n",
            "[50]\ttraining's binary_logloss: 0.0834516\tvalid_1's binary_logloss: 0.315121\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's binary_logloss: 0.11492\tvalid_1's binary_logloss: 0.306746\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.345836\tvalid_1's binary_logloss: 0.420032\n",
            "[20]\ttraining's binary_logloss: 0.221325\tvalid_1's binary_logloss: 0.363038\n",
            "[30]\ttraining's binary_logloss: 0.153173\tvalid_1's binary_logloss: 0.350717\n",
            "Early stopping, best iteration is:\n",
            "[29]\ttraining's binary_logloss: 0.158445\tvalid_1's binary_logloss: 0.347504\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.354737\tvalid_1's binary_logloss: 0.422962\n",
            "[20]\ttraining's binary_logloss: 0.23276\tvalid_1's binary_logloss: 0.344597\n",
            "[30]\ttraining's binary_logloss: 0.166214\tvalid_1's binary_logloss: 0.331226\n",
            "[40]\ttraining's binary_logloss: 0.12263\tvalid_1's binary_logloss: 0.327313\n",
            "[50]\ttraining's binary_logloss: 0.0886418\tvalid_1's binary_logloss: 0.332912\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.111478\tvalid_1's binary_logloss: 0.327063\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.345984\tvalid_1's binary_logloss: 0.404722\n",
            "[20]\ttraining's binary_logloss: 0.225945\tvalid_1's binary_logloss: 0.325619\n",
            "[30]\ttraining's binary_logloss: 0.158844\tvalid_1's binary_logloss: 0.292428\n",
            "[40]\ttraining's binary_logloss: 0.112927\tvalid_1's binary_logloss: 0.285572\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.125346\tvalid_1's binary_logloss: 0.283413\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352531\tvalid_1's binary_logloss: 0.390154\n",
            "[20]\ttraining's binary_logloss: 0.230987\tvalid_1's binary_logloss: 0.311365\n",
            "[30]\ttraining's binary_logloss: 0.162819\tvalid_1's binary_logloss: 0.280519\n",
            "[40]\ttraining's binary_logloss: 0.115951\tvalid_1's binary_logloss: 0.273733\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's binary_logloss: 0.123171\tvalid_1's binary_logloss: 0.271774\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.347255\tvalid_1's binary_logloss: 0.435199\n",
            "[20]\ttraining's binary_logloss: 0.217902\tvalid_1's binary_logloss: 0.38314\n",
            "[30]\ttraining's binary_logloss: 0.152411\tvalid_1's binary_logloss: 0.37982\n",
            "Early stopping, best iteration is:\n",
            "[26]\ttraining's binary_logloss: 0.176059\tvalid_1's binary_logloss: 0.374173\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342346\tvalid_1's binary_logloss: 0.421127\n",
            "[20]\ttraining's binary_logloss: 0.222131\tvalid_1's binary_logloss: 0.359943\n",
            "[30]\ttraining's binary_logloss: 0.153971\tvalid_1's binary_logloss: 0.35168\n",
            "[40]\ttraining's binary_logloss: 0.108631\tvalid_1's binary_logloss: 0.349415\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.143293\tvalid_1's binary_logloss: 0.34678\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352432\tvalid_1's binary_logloss: 0.416846\n",
            "[20]\ttraining's binary_logloss: 0.232993\tvalid_1's binary_logloss: 0.343751\n",
            "[30]\ttraining's binary_logloss: 0.165179\tvalid_1's binary_logloss: 0.327744\n",
            "[40]\ttraining's binary_logloss: 0.117891\tvalid_1's binary_logloss: 0.329328\n",
            "Early stopping, best iteration is:\n",
            "[34]\ttraining's binary_logloss: 0.142719\tvalid_1's binary_logloss: 0.323371\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.349639\tvalid_1's binary_logloss: 0.426195\n",
            "[20]\ttraining's binary_logloss: 0.22249\tvalid_1's binary_logloss: 0.345066\n",
            "[30]\ttraining's binary_logloss: 0.155191\tvalid_1's binary_logloss: 0.316352\n",
            "[40]\ttraining's binary_logloss: 0.109197\tvalid_1's binary_logloss: 0.314229\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.125216\tvalid_1's binary_logloss: 0.309455\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.34824\tvalid_1's binary_logloss: 0.407453\n",
            "[20]\ttraining's binary_logloss: 0.230139\tvalid_1's binary_logloss: 0.338413\n",
            "[30]\ttraining's binary_logloss: 0.163721\tvalid_1's binary_logloss: 0.323898\n",
            "[40]\ttraining's binary_logloss: 0.117286\tvalid_1's binary_logloss: 0.325606\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.152981\tvalid_1's binary_logloss: 0.322052\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.343481\tvalid_1's binary_logloss: 0.41169\n",
            "[20]\ttraining's binary_logloss: 0.219281\tvalid_1's binary_logloss: 0.343272\n",
            "[30]\ttraining's binary_logloss: 0.154037\tvalid_1's binary_logloss: 0.334052\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.163712\tvalid_1's binary_logloss: 0.330325\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.359559\tvalid_1's binary_logloss: 0.382444\n",
            "[20]\ttraining's binary_logloss: 0.236998\tvalid_1's binary_logloss: 0.307646\n",
            "[30]\ttraining's binary_logloss: 0.169569\tvalid_1's binary_logloss: 0.277557\n",
            "[40]\ttraining's binary_logloss: 0.123298\tvalid_1's binary_logloss: 0.27682\n",
            "[50]\ttraining's binary_logloss: 0.090021\tvalid_1's binary_logloss: 0.27674\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.111689\tvalid_1's binary_logloss: 0.273407\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.35079\tvalid_1's binary_logloss: 0.399104\n",
            "[20]\ttraining's binary_logloss: 0.225528\tvalid_1's binary_logloss: 0.318346\n",
            "[30]\ttraining's binary_logloss: 0.161849\tvalid_1's binary_logloss: 0.304057\n",
            "[40]\ttraining's binary_logloss: 0.117517\tvalid_1's binary_logloss: 0.300721\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's binary_logloss: 0.137035\tvalid_1's binary_logloss: 0.296209\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.350536\tvalid_1's binary_logloss: 0.39413\n",
            "[20]\ttraining's binary_logloss: 0.229369\tvalid_1's binary_logloss: 0.318772\n",
            "[30]\ttraining's binary_logloss: 0.159451\tvalid_1's binary_logloss: 0.296395\n",
            "[40]\ttraining's binary_logloss: 0.112922\tvalid_1's binary_logloss: 0.284424\n",
            "[50]\ttraining's binary_logloss: 0.0798604\tvalid_1's binary_logloss: 0.289747\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's binary_logloss: 0.0989374\tvalid_1's binary_logloss: 0.282404\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.34453\tvalid_1's binary_logloss: 0.429516\n",
            "[20]\ttraining's binary_logloss: 0.223299\tvalid_1's binary_logloss: 0.364915\n",
            "[30]\ttraining's binary_logloss: 0.155993\tvalid_1's binary_logloss: 0.341712\n",
            "[40]\ttraining's binary_logloss: 0.109982\tvalid_1's binary_logloss: 0.31667\n",
            "[50]\ttraining's binary_logloss: 0.0780842\tvalid_1's binary_logloss: 0.318272\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's binary_logloss: 0.10591\tvalid_1's binary_logloss: 0.315875\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.343648\tvalid_1's binary_logloss: 0.411092\n",
            "[20]\ttraining's binary_logloss: 0.230328\tvalid_1's binary_logloss: 0.329818\n",
            "[30]\ttraining's binary_logloss: 0.165243\tvalid_1's binary_logloss: 0.301826\n",
            "[40]\ttraining's binary_logloss: 0.121309\tvalid_1's binary_logloss: 0.298461\n",
            "[50]\ttraining's binary_logloss: 0.0881808\tvalid_1's binary_logloss: 0.290781\n",
            "[60]\ttraining's binary_logloss: 0.0627574\tvalid_1's binary_logloss: 0.294119\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's binary_logloss: 0.0881808\tvalid_1's binary_logloss: 0.290781\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344763\tvalid_1's binary_logloss: 0.417746\n",
            "[20]\ttraining's binary_logloss: 0.223664\tvalid_1's binary_logloss: 0.35875\n",
            "[30]\ttraining's binary_logloss: 0.156225\tvalid_1's binary_logloss: 0.345967\n",
            "Early stopping, best iteration is:\n",
            "[29]\ttraining's binary_logloss: 0.161312\tvalid_1's binary_logloss: 0.342609\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.337811\tvalid_1's binary_logloss: 0.407051\n",
            "[20]\ttraining's binary_logloss: 0.215569\tvalid_1's binary_logloss: 0.344219\n",
            "[30]\ttraining's binary_logloss: 0.154099\tvalid_1's binary_logloss: 0.315246\n",
            "[40]\ttraining's binary_logloss: 0.110977\tvalid_1's binary_logloss: 0.310473\n",
            "[50]\ttraining's binary_logloss: 0.0807872\tvalid_1's binary_logloss: 0.31328\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's binary_logloss: 0.107656\tvalid_1's binary_logloss: 0.308744\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352875\tvalid_1's binary_logloss: 0.40398\n",
            "[20]\ttraining's binary_logloss: 0.23114\tvalid_1's binary_logloss: 0.328369\n",
            "[30]\ttraining's binary_logloss: 0.164241\tvalid_1's binary_logloss: 0.29577\n",
            "[40]\ttraining's binary_logloss: 0.1211\tvalid_1's binary_logloss: 0.280137\n",
            "[50]\ttraining's binary_logloss: 0.0882044\tvalid_1's binary_logloss: 0.270175\n",
            "[60]\ttraining's binary_logloss: 0.0634115\tvalid_1's binary_logloss: 0.271585\n",
            "Early stopping, best iteration is:\n",
            "[51]\ttraining's binary_logloss: 0.0857083\tvalid_1's binary_logloss: 0.268376\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.340353\tvalid_1's binary_logloss: 0.431903\n",
            "[20]\ttraining's binary_logloss: 0.218081\tvalid_1's binary_logloss: 0.360036\n",
            "[30]\ttraining's binary_logloss: 0.154643\tvalid_1's binary_logloss: 0.335089\n",
            "[40]\ttraining's binary_logloss: 0.109478\tvalid_1's binary_logloss: 0.331543\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.12589\tvalid_1's binary_logloss: 0.324921\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.3392\tvalid_1's binary_logloss: 0.434057\n",
            "[20]\ttraining's binary_logloss: 0.218066\tvalid_1's binary_logloss: 0.381407\n",
            "[30]\ttraining's binary_logloss: 0.149538\tvalid_1's binary_logloss: 0.37358\n",
            "Early stopping, best iteration is:\n",
            "[24]\ttraining's binary_logloss: 0.187762\tvalid_1's binary_logloss: 0.372379\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.362438\tvalid_1's binary_logloss: 0.3926\n",
            "[20]\ttraining's binary_logloss: 0.236326\tvalid_1's binary_logloss: 0.31702\n",
            "[30]\ttraining's binary_logloss: 0.171701\tvalid_1's binary_logloss: 0.287345\n",
            "[40]\ttraining's binary_logloss: 0.124502\tvalid_1's binary_logloss: 0.267738\n",
            "[50]\ttraining's binary_logloss: 0.0891951\tvalid_1's binary_logloss: 0.267455\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's binary_logloss: 0.102011\tvalid_1's binary_logloss: 0.264785\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.356208\tvalid_1's binary_logloss: 0.400712\n",
            "[20]\ttraining's binary_logloss: 0.232651\tvalid_1's binary_logloss: 0.317399\n",
            "[30]\ttraining's binary_logloss: 0.164179\tvalid_1's binary_logloss: 0.290176\n",
            "[40]\ttraining's binary_logloss: 0.121789\tvalid_1's binary_logloss: 0.283632\n",
            "[50]\ttraining's binary_logloss: 0.0874541\tvalid_1's binary_logloss: 0.280594\n",
            "[60]\ttraining's binary_logloss: 0.0636342\tvalid_1's binary_logloss: 0.284171\n",
            "Early stopping, best iteration is:\n",
            "[52]\ttraining's binary_logloss: 0.0815405\tvalid_1's binary_logloss: 0.280144\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342985\tvalid_1's binary_logloss: 0.426627\n",
            "[20]\ttraining's binary_logloss: 0.224499\tvalid_1's binary_logloss: 0.357176\n",
            "[30]\ttraining's binary_logloss: 0.160776\tvalid_1's binary_logloss: 0.335827\n",
            "[40]\ttraining's binary_logloss: 0.113637\tvalid_1's binary_logloss: 0.331503\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.117814\tvalid_1's binary_logloss: 0.329048\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.348072\tvalid_1's binary_logloss: 0.418626\n",
            "[20]\ttraining's binary_logloss: 0.227684\tvalid_1's binary_logloss: 0.347781\n",
            "[30]\ttraining's binary_logloss: 0.155766\tvalid_1's binary_logloss: 0.315697\n",
            "[40]\ttraining's binary_logloss: 0.109655\tvalid_1's binary_logloss: 0.307078\n",
            "[50]\ttraining's binary_logloss: 0.0787131\tvalid_1's binary_logloss: 0.311328\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's binary_logloss: 0.0953111\tvalid_1's binary_logloss: 0.306677\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.359013\tvalid_1's binary_logloss: 0.383548\n",
            "[20]\ttraining's binary_logloss: 0.235117\tvalid_1's binary_logloss: 0.305707\n",
            "[30]\ttraining's binary_logloss: 0.164105\tvalid_1's binary_logloss: 0.289496\n",
            "[40]\ttraining's binary_logloss: 0.115711\tvalid_1's binary_logloss: 0.281654\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.127057\tvalid_1's binary_logloss: 0.277587\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.34715\tvalid_1's binary_logloss: 0.403497\n",
            "[20]\ttraining's binary_logloss: 0.223181\tvalid_1's binary_logloss: 0.335973\n",
            "[30]\ttraining's binary_logloss: 0.157244\tvalid_1's binary_logloss: 0.319348\n",
            "[40]\ttraining's binary_logloss: 0.115407\tvalid_1's binary_logloss: 0.317211\n",
            "Early stopping, best iteration is:\n",
            "[34]\ttraining's binary_logloss: 0.137756\tvalid_1's binary_logloss: 0.312208\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.350346\tvalid_1's binary_logloss: 0.402797\n",
            "[20]\ttraining's binary_logloss: 0.227239\tvalid_1's binary_logloss: 0.338086\n",
            "[30]\ttraining's binary_logloss: 0.160234\tvalid_1's binary_logloss: 0.322978\n",
            "[40]\ttraining's binary_logloss: 0.114271\tvalid_1's binary_logloss: 0.323038\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.148851\tvalid_1's binary_logloss: 0.322162\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.353801\tvalid_1's binary_logloss: 0.405676\n",
            "[20]\ttraining's binary_logloss: 0.229661\tvalid_1's binary_logloss: 0.329928\n",
            "[30]\ttraining's binary_logloss: 0.164047\tvalid_1's binary_logloss: 0.300998\n",
            "[40]\ttraining's binary_logloss: 0.119765\tvalid_1's binary_logloss: 0.290581\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.123874\tvalid_1's binary_logloss: 0.289806\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.345986\tvalid_1's binary_logloss: 0.396807\n",
            "[20]\ttraining's binary_logloss: 0.221177\tvalid_1's binary_logloss: 0.32074\n",
            "[30]\ttraining's binary_logloss: 0.154337\tvalid_1's binary_logloss: 0.302828\n",
            "[40]\ttraining's binary_logloss: 0.110356\tvalid_1's binary_logloss: 0.287927\n",
            "[50]\ttraining's binary_logloss: 0.0778813\tvalid_1's binary_logloss: 0.293685\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.099072\tvalid_1's binary_logloss: 0.287524\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.339957\tvalid_1's binary_logloss: 0.437335\n",
            "[20]\ttraining's binary_logloss: 0.216442\tvalid_1's binary_logloss: 0.382137\n",
            "[30]\ttraining's binary_logloss: 0.14776\tvalid_1's binary_logloss: 0.369268\n",
            "[40]\ttraining's binary_logloss: 0.108623\tvalid_1's binary_logloss: 0.37859\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.138425\tvalid_1's binary_logloss: 0.368487\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344567\tvalid_1's binary_logloss: 0.420993\n",
            "[20]\ttraining's binary_logloss: 0.217556\tvalid_1's binary_logloss: 0.367075\n",
            "[30]\ttraining's binary_logloss: 0.149358\tvalid_1's binary_logloss: 0.351591\n",
            "[40]\ttraining's binary_logloss: 0.105313\tvalid_1's binary_logloss: 0.349607\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.109396\tvalid_1's binary_logloss: 0.346186\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.341539\tvalid_1's binary_logloss: 0.447884\n",
            "[20]\ttraining's binary_logloss: 0.217937\tvalid_1's binary_logloss: 0.389965\n",
            "[30]\ttraining's binary_logloss: 0.150687\tvalid_1's binary_logloss: 0.371919\n",
            "[40]\ttraining's binary_logloss: 0.104569\tvalid_1's binary_logloss: 0.368066\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.121693\tvalid_1's binary_logloss: 0.365277\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.359924\tvalid_1's binary_logloss: 0.433432\n",
            "[20]\ttraining's binary_logloss: 0.235165\tvalid_1's binary_logloss: 0.355204\n",
            "[30]\ttraining's binary_logloss: 0.162728\tvalid_1's binary_logloss: 0.321345\n",
            "[40]\ttraining's binary_logloss: 0.1165\tvalid_1's binary_logloss: 0.314623\n",
            "[50]\ttraining's binary_logloss: 0.0828296\tvalid_1's binary_logloss: 0.314427\n",
            "Early stopping, best iteration is:\n",
            "[46]\ttraining's binary_logloss: 0.0940321\tvalid_1's binary_logloss: 0.309021\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346722\tvalid_1's binary_logloss: 0.433768\n",
            "[20]\ttraining's binary_logloss: 0.218452\tvalid_1's binary_logloss: 0.359716\n",
            "[30]\ttraining's binary_logloss: 0.148865\tvalid_1's binary_logloss: 0.344815\n",
            "[40]\ttraining's binary_logloss: 0.103411\tvalid_1's binary_logloss: 0.34917\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.138387\tvalid_1's binary_logloss: 0.340913\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346743\tvalid_1's binary_logloss: 0.415187\n",
            "[20]\ttraining's binary_logloss: 0.228802\tvalid_1's binary_logloss: 0.346784\n",
            "[30]\ttraining's binary_logloss: 0.158263\tvalid_1's binary_logloss: 0.336922\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.171447\tvalid_1's binary_logloss: 0.333732\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.348029\tvalid_1's binary_logloss: 0.41154\n",
            "[20]\ttraining's binary_logloss: 0.222715\tvalid_1's binary_logloss: 0.341378\n",
            "[30]\ttraining's binary_logloss: 0.155962\tvalid_1's binary_logloss: 0.328348\n",
            "[40]\ttraining's binary_logloss: 0.110919\tvalid_1's binary_logloss: 0.33072\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.144661\tvalid_1's binary_logloss: 0.327363\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.348843\tvalid_1's binary_logloss: 0.404309\n",
            "[20]\ttraining's binary_logloss: 0.227479\tvalid_1's binary_logloss: 0.330136\n",
            "[30]\ttraining's binary_logloss: 0.158847\tvalid_1's binary_logloss: 0.302563\n",
            "[40]\ttraining's binary_logloss: 0.112522\tvalid_1's binary_logloss: 0.30818\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.12458\tvalid_1's binary_logloss: 0.300845\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342715\tvalid_1's binary_logloss: 0.424154\n",
            "[20]\ttraining's binary_logloss: 0.220755\tvalid_1's binary_logloss: 0.357858\n",
            "[30]\ttraining's binary_logloss: 0.154293\tvalid_1's binary_logloss: 0.333492\n",
            "[40]\ttraining's binary_logloss: 0.111899\tvalid_1's binary_logloss: 0.328835\n",
            "Early stopping, best iteration is:\n",
            "[34]\ttraining's binary_logloss: 0.136092\tvalid_1's binary_logloss: 0.325831\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.351373\tvalid_1's binary_logloss: 0.399504\n",
            "[20]\ttraining's binary_logloss: 0.225189\tvalid_1's binary_logloss: 0.333748\n",
            "[30]\ttraining's binary_logloss: 0.156114\tvalid_1's binary_logloss: 0.324392\n",
            "[40]\ttraining's binary_logloss: 0.111535\tvalid_1's binary_logloss: 0.338142\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.146134\tvalid_1's binary_logloss: 0.320904\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344804\tvalid_1's binary_logloss: 0.419551\n",
            "[20]\ttraining's binary_logloss: 0.231117\tvalid_1's binary_logloss: 0.338093\n",
            "[30]\ttraining's binary_logloss: 0.162735\tvalid_1's binary_logloss: 0.309529\n",
            "[40]\ttraining's binary_logloss: 0.113536\tvalid_1's binary_logloss: 0.300346\n",
            "[50]\ttraining's binary_logloss: 0.0815509\tvalid_1's binary_logloss: 0.305597\n",
            "Early stopping, best iteration is:\n",
            "[41]\ttraining's binary_logloss: 0.10942\tvalid_1's binary_logloss: 0.299705\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.339847\tvalid_1's binary_logloss: 0.428389\n",
            "[20]\ttraining's binary_logloss: 0.222467\tvalid_1's binary_logloss: 0.371152\n",
            "[30]\ttraining's binary_logloss: 0.160984\tvalid_1's binary_logloss: 0.355514\n",
            "[40]\ttraining's binary_logloss: 0.116668\tvalid_1's binary_logloss: 0.348536\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's binary_logloss: 0.124414\tvalid_1's binary_logloss: 0.345229\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.35262\tvalid_1's binary_logloss: 0.397916\n",
            "[20]\ttraining's binary_logloss: 0.233193\tvalid_1's binary_logloss: 0.312516\n",
            "[30]\ttraining's binary_logloss: 0.169824\tvalid_1's binary_logloss: 0.289761\n",
            "[40]\ttraining's binary_logloss: 0.12286\tvalid_1's binary_logloss: 0.281368\n",
            "[50]\ttraining's binary_logloss: 0.0902183\tvalid_1's binary_logloss: 0.288516\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's binary_logloss: 0.12286\tvalid_1's binary_logloss: 0.281368\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346493\tvalid_1's binary_logloss: 0.424127\n",
            "[20]\ttraining's binary_logloss: 0.221258\tvalid_1's binary_logloss: 0.363702\n",
            "[30]\ttraining's binary_logloss: 0.155909\tvalid_1's binary_logloss: 0.353923\n",
            "[40]\ttraining's binary_logloss: 0.111438\tvalid_1's binary_logloss: 0.339685\n",
            "[50]\ttraining's binary_logloss: 0.0791104\tvalid_1's binary_logloss: 0.338588\n",
            "[60]\ttraining's binary_logloss: 0.0563647\tvalid_1's binary_logloss: 0.342619\n",
            "Early stopping, best iteration is:\n",
            "[50]\ttraining's binary_logloss: 0.0791104\tvalid_1's binary_logloss: 0.338588\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.357227\tvalid_1's binary_logloss: 0.389815\n",
            "[20]\ttraining's binary_logloss: 0.235855\tvalid_1's binary_logloss: 0.309794\n",
            "[30]\ttraining's binary_logloss: 0.167739\tvalid_1's binary_logloss: 0.277175\n",
            "[40]\ttraining's binary_logloss: 0.120337\tvalid_1's binary_logloss: 0.274308\n",
            "[50]\ttraining's binary_logloss: 0.0859999\tvalid_1's binary_logloss: 0.273968\n",
            "Early stopping, best iteration is:\n",
            "[47]\ttraining's binary_logloss: 0.0942004\tvalid_1's binary_logloss: 0.270262\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352248\tvalid_1's binary_logloss: 0.409297\n",
            "[20]\ttraining's binary_logloss: 0.228002\tvalid_1's binary_logloss: 0.345713\n",
            "[30]\ttraining's binary_logloss: 0.154846\tvalid_1's binary_logloss: 0.315085\n",
            "[40]\ttraining's binary_logloss: 0.11008\tvalid_1's binary_logloss: 0.310025\n",
            "Early stopping, best iteration is:\n",
            "[39]\ttraining's binary_logloss: 0.113807\tvalid_1's binary_logloss: 0.308805\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.357301\tvalid_1's binary_logloss: 0.397509\n",
            "[20]\ttraining's binary_logloss: 0.23356\tvalid_1's binary_logloss: 0.319699\n",
            "[30]\ttraining's binary_logloss: 0.165585\tvalid_1's binary_logloss: 0.296013\n",
            "[40]\ttraining's binary_logloss: 0.119443\tvalid_1's binary_logloss: 0.285847\n",
            "[50]\ttraining's binary_logloss: 0.0868224\tvalid_1's binary_logloss: 0.288275\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.108299\tvalid_1's binary_logloss: 0.283846\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.357477\tvalid_1's binary_logloss: 0.386279\n",
            "[20]\ttraining's binary_logloss: 0.239159\tvalid_1's binary_logloss: 0.302197\n",
            "[30]\ttraining's binary_logloss: 0.167055\tvalid_1's binary_logloss: 0.282574\n",
            "[40]\ttraining's binary_logloss: 0.118132\tvalid_1's binary_logloss: 0.284711\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's binary_logloss: 0.139874\tvalid_1's binary_logloss: 0.280233\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344411\tvalid_1's binary_logloss: 0.418718\n",
            "[20]\ttraining's binary_logloss: 0.225167\tvalid_1's binary_logloss: 0.341021\n",
            "[30]\ttraining's binary_logloss: 0.156496\tvalid_1's binary_logloss: 0.311591\n",
            "Early stopping, best iteration is:\n",
            "[29]\ttraining's binary_logloss: 0.162306\tvalid_1's binary_logloss: 0.309024\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346101\tvalid_1's binary_logloss: 0.421424\n",
            "[20]\ttraining's binary_logloss: 0.216149\tvalid_1's binary_logloss: 0.356821\n",
            "[30]\ttraining's binary_logloss: 0.147851\tvalid_1's binary_logloss: 0.347768\n",
            "Early stopping, best iteration is:\n",
            "[29]\ttraining's binary_logloss: 0.153252\tvalid_1's binary_logloss: 0.346327\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.350153\tvalid_1's binary_logloss: 0.396073\n",
            "[20]\ttraining's binary_logloss: 0.227\tvalid_1's binary_logloss: 0.322449\n",
            "[30]\ttraining's binary_logloss: 0.160407\tvalid_1's binary_logloss: 0.298384\n",
            "[40]\ttraining's binary_logloss: 0.115205\tvalid_1's binary_logloss: 0.293407\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.126707\tvalid_1's binary_logloss: 0.292419\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.343699\tvalid_1's binary_logloss: 0.418285\n",
            "[20]\ttraining's binary_logloss: 0.220072\tvalid_1's binary_logloss: 0.350983\n",
            "[30]\ttraining's binary_logloss: 0.153954\tvalid_1's binary_logloss: 0.337981\n",
            "[40]\ttraining's binary_logloss: 0.109589\tvalid_1's binary_logloss: 0.342202\n",
            "Early stopping, best iteration is:\n",
            "[35]\ttraining's binary_logloss: 0.130211\tvalid_1's binary_logloss: 0.335739\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.342867\tvalid_1's binary_logloss: 0.438358\n",
            "[20]\ttraining's binary_logloss: 0.221367\tvalid_1's binary_logloss: 0.377245\n",
            "[30]\ttraining's binary_logloss: 0.155926\tvalid_1's binary_logloss: 0.369112\n",
            "[40]\ttraining's binary_logloss: 0.111498\tvalid_1's binary_logloss: 0.373568\n",
            "Early stopping, best iteration is:\n",
            "[30]\ttraining's binary_logloss: 0.155926\tvalid_1's binary_logloss: 0.369112\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346327\tvalid_1's binary_logloss: 0.40139\n",
            "[20]\ttraining's binary_logloss: 0.229075\tvalid_1's binary_logloss: 0.341796\n",
            "[30]\ttraining's binary_logloss: 0.157597\tvalid_1's binary_logloss: 0.332926\n",
            "Early stopping, best iteration is:\n",
            "[26]\ttraining's binary_logloss: 0.182528\tvalid_1's binary_logloss: 0.331815\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.34532\tvalid_1's binary_logloss: 0.419674\n",
            "[20]\ttraining's binary_logloss: 0.22744\tvalid_1's binary_logloss: 0.359148\n",
            "[30]\ttraining's binary_logloss: 0.158345\tvalid_1's binary_logloss: 0.343086\n",
            "[40]\ttraining's binary_logloss: 0.110235\tvalid_1's binary_logloss: 0.340097\n",
            "Early stopping, best iteration is:\n",
            "[37]\ttraining's binary_logloss: 0.123152\tvalid_1's binary_logloss: 0.337117\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.352251\tvalid_1's binary_logloss: 0.39812\n",
            "[20]\ttraining's binary_logloss: 0.233985\tvalid_1's binary_logloss: 0.315121\n",
            "[30]\ttraining's binary_logloss: 0.167629\tvalid_1's binary_logloss: 0.282234\n",
            "[40]\ttraining's binary_logloss: 0.122694\tvalid_1's binary_logloss: 0.268314\n",
            "[50]\ttraining's binary_logloss: 0.0889564\tvalid_1's binary_logloss: 0.259973\n",
            "[60]\ttraining's binary_logloss: 0.0641913\tvalid_1's binary_logloss: 0.25528\n",
            "[70]\ttraining's binary_logloss: 0.047008\tvalid_1's binary_logloss: 0.259382\n",
            "Early stopping, best iteration is:\n",
            "[60]\ttraining's binary_logloss: 0.0641913\tvalid_1's binary_logloss: 0.25528\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.354876\tvalid_1's binary_logloss: 0.398398\n",
            "[20]\ttraining's binary_logloss: 0.232403\tvalid_1's binary_logloss: 0.329587\n",
            "[30]\ttraining's binary_logloss: 0.163339\tvalid_1's binary_logloss: 0.305567\n",
            "[40]\ttraining's binary_logloss: 0.118856\tvalid_1's binary_logloss: 0.299826\n",
            "[50]\ttraining's binary_logloss: 0.0870046\tvalid_1's binary_logloss: 0.300232\n",
            "Early stopping, best iteration is:\n",
            "[43]\ttraining's binary_logloss: 0.107766\tvalid_1's binary_logloss: 0.296864\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.344705\tvalid_1's binary_logloss: 0.424308\n",
            "[20]\ttraining's binary_logloss: 0.225522\tvalid_1's binary_logloss: 0.36507\n",
            "[30]\ttraining's binary_logloss: 0.1616\tvalid_1's binary_logloss: 0.343185\n",
            "[40]\ttraining's binary_logloss: 0.113782\tvalid_1's binary_logloss: 0.33272\n",
            "[50]\ttraining's binary_logloss: 0.0819978\tvalid_1's binary_logloss: 0.335786\n",
            "Early stopping, best iteration is:\n",
            "[45]\ttraining's binary_logloss: 0.0966964\tvalid_1's binary_logloss: 0.331193\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.341527\tvalid_1's binary_logloss: 0.440189\n",
            "[20]\ttraining's binary_logloss: 0.214008\tvalid_1's binary_logloss: 0.383866\n",
            "[30]\ttraining's binary_logloss: 0.146178\tvalid_1's binary_logloss: 0.375521\n",
            "Early stopping, best iteration is:\n",
            "[26]\ttraining's binary_logloss: 0.168923\tvalid_1's binary_logloss: 0.371882\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.346224\tvalid_1's binary_logloss: 0.413168\n",
            "[20]\ttraining's binary_logloss: 0.221563\tvalid_1's binary_logloss: 0.342096\n",
            "[30]\ttraining's binary_logloss: 0.155018\tvalid_1's binary_logloss: 0.331651\n",
            "Early stopping, best iteration is:\n",
            "[27]\ttraining's binary_logloss: 0.171823\tvalid_1's binary_logloss: 0.330762\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.345919\tvalid_1's binary_logloss: 0.395577\n",
            "[20]\ttraining's binary_logloss: 0.224963\tvalid_1's binary_logloss: 0.330261\n",
            "[30]\ttraining's binary_logloss: 0.159892\tvalid_1's binary_logloss: 0.316699\n",
            "[40]\ttraining's binary_logloss: 0.114857\tvalid_1's binary_logloss: 0.314196\n",
            "[50]\ttraining's binary_logloss: 0.0849045\tvalid_1's binary_logloss: 0.327701\n",
            "Early stopping, best iteration is:\n",
            "[40]\ttraining's binary_logloss: 0.114857\tvalid_1's binary_logloss: 0.314196\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.3431\tvalid_1's binary_logloss: 0.419825\n",
            "[20]\ttraining's binary_logloss: 0.224338\tvalid_1's binary_logloss: 0.357544\n",
            "[30]\ttraining's binary_logloss: 0.15633\tvalid_1's binary_logloss: 0.342762\n",
            "Early stopping, best iteration is:\n",
            "[28]\ttraining's binary_logloss: 0.16821\tvalid_1's binary_logloss: 0.341371\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.348178\tvalid_1's binary_logloss: 0.429356\n",
            "[20]\ttraining's binary_logloss: 0.224284\tvalid_1's binary_logloss: 0.358396\n",
            "[30]\ttraining's binary_logloss: 0.158298\tvalid_1's binary_logloss: 0.337552\n",
            "[40]\ttraining's binary_logloss: 0.114354\tvalid_1's binary_logloss: 0.3463\n",
            "Early stopping, best iteration is:\n",
            "[31]\ttraining's binary_logloss: 0.153284\tvalid_1's binary_logloss: 0.335977\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.343539\tvalid_1's binary_logloss: 0.440789\n",
            "[20]\ttraining's binary_logloss: 0.223161\tvalid_1's binary_logloss: 0.390097\n",
            "[30]\ttraining's binary_logloss: 0.157252\tvalid_1's binary_logloss: 0.361884\n",
            "[40]\ttraining's binary_logloss: 0.109611\tvalid_1's binary_logloss: 0.364521\n",
            "Early stopping, best iteration is:\n",
            "[32]\ttraining's binary_logloss: 0.147035\tvalid_1's binary_logloss: 0.358382\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.357835\tvalid_1's binary_logloss: 0.3794\n",
            "[20]\ttraining's binary_logloss: 0.241818\tvalid_1's binary_logloss: 0.293688\n",
            "[30]\ttraining's binary_logloss: 0.171894\tvalid_1's binary_logloss: 0.268043\n",
            "[40]\ttraining's binary_logloss: 0.127255\tvalid_1's binary_logloss: 0.264627\n",
            "[50]\ttraining's binary_logloss: 0.0936192\tvalid_1's binary_logloss: 0.263839\n",
            "Early stopping, best iteration is:\n",
            "[44]\ttraining's binary_logloss: 0.112498\tvalid_1's binary_logloss: 0.263661\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.351839\tvalid_1's binary_logloss: 0.406936\n",
            "[20]\ttraining's binary_logloss: 0.234238\tvalid_1's binary_logloss: 0.335188\n",
            "[30]\ttraining's binary_logloss: 0.166322\tvalid_1's binary_logloss: 0.314236\n",
            "[40]\ttraining's binary_logloss: 0.119124\tvalid_1's binary_logloss: 0.308504\n",
            "Early stopping, best iteration is:\n",
            "[38]\ttraining's binary_logloss: 0.12748\tvalid_1's binary_logloss: 0.308271\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.362568\tvalid_1's binary_logloss: 0.38911\n",
            "[20]\ttraining's binary_logloss: 0.244078\tvalid_1's binary_logloss: 0.297447\n",
            "[30]\ttraining's binary_logloss: 0.173943\tvalid_1's binary_logloss: 0.266763\n",
            "[40]\ttraining's binary_logloss: 0.1264\tvalid_1's binary_logloss: 0.265494\n",
            "Early stopping, best iteration is:\n",
            "[36]\ttraining's binary_logloss: 0.144049\tvalid_1's binary_logloss: 0.262414\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.345976\tvalid_1's binary_logloss: 0.423363\n",
            "[20]\ttraining's binary_logloss: 0.223807\tvalid_1's binary_logloss: 0.356985\n",
            "[30]\ttraining's binary_logloss: 0.158201\tvalid_1's binary_logloss: 0.344694\n",
            "Early stopping, best iteration is:\n",
            "[25]\ttraining's binary_logloss: 0.188255\tvalid_1's binary_logloss: 0.342199\n",
            "Training until validation scores don't improve for 10 rounds.\n",
            "[10]\ttraining's binary_logloss: 0.340385\tvalid_1's binary_logloss: 0.407559\n",
            "[20]\ttraining's binary_logloss: 0.220246\tvalid_1's binary_logloss: 0.335409\n",
            "[30]\ttraining's binary_logloss: 0.154284\tvalid_1's binary_logloss: 0.323381\n",
            "[40]\ttraining's binary_logloss: 0.108567\tvalid_1's binary_logloss: 0.328771\n",
            "Early stopping, best iteration is:\n",
            "[30]\ttraining's binary_logloss: 0.154284\tvalid_1's binary_logloss: 0.323381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3woi3Z9bKJTo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9ded8dab-bf07-4d7e-b256-c17285a5b9f9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "percentage_LGB=0.3\n",
        "y_pred_fin = output_df.mean(axis = 1)\n",
        "#y_sub_LGB = sum(y_preds) / len(y_preds)\n",
        "y_sub = (y_pred > percentage_LGB).astype(int)\n",
        "\n",
        "submit_df[\"B\"]=y_sub\n",
        "submit_df.to_csv('submit_test.csv', index=False, header=False)\n",
        "submit_df.head()\n",
        "\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>B</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   A  B\n",
              "0  0  0\n",
              "1  1  1\n",
              "2  2  0\n",
              "3  3  0\n",
              "4  4  0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELHU_EDIPBmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "5bb7205b-034a-4492-971c-d2439377e04c"
      },
      "source": [
        "output_df"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.004702</td>\n",
              "      <td>0.021260</td>\n",
              "      <td>0.008639</td>\n",
              "      <td>0.015738</td>\n",
              "      <td>0.009862</td>\n",
              "      <td>0.023914</td>\n",
              "      <td>0.032604</td>\n",
              "      <td>0.011886</td>\n",
              "      <td>0.017469</td>\n",
              "      <td>0.017155</td>\n",
              "      <td>0.006398</td>\n",
              "      <td>0.012180</td>\n",
              "      <td>0.009505</td>\n",
              "      <td>0.040283</td>\n",
              "      <td>0.020470</td>\n",
              "      <td>0.028410</td>\n",
              "      <td>0.023151</td>\n",
              "      <td>0.044290</td>\n",
              "      <td>0.015008</td>\n",
              "      <td>0.029325</td>\n",
              "      <td>0.016248</td>\n",
              "      <td>0.026531</td>\n",
              "      <td>0.028259</td>\n",
              "      <td>0.012811</td>\n",
              "      <td>0.013008</td>\n",
              "      <td>0.047603</td>\n",
              "      <td>0.008486</td>\n",
              "      <td>0.016892</td>\n",
              "      <td>0.031181</td>\n",
              "      <td>0.023770</td>\n",
              "      <td>0.012532</td>\n",
              "      <td>0.017247</td>\n",
              "      <td>0.029692</td>\n",
              "      <td>0.014029</td>\n",
              "      <td>0.011198</td>\n",
              "      <td>0.010578</td>\n",
              "      <td>0.036858</td>\n",
              "      <td>0.021084</td>\n",
              "      <td>0.016156</td>\n",
              "      <td>0.017488</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008881</td>\n",
              "      <td>0.024527</td>\n",
              "      <td>0.014314</td>\n",
              "      <td>0.015825</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>0.025841</td>\n",
              "      <td>0.035333</td>\n",
              "      <td>0.019956</td>\n",
              "      <td>0.014118</td>\n",
              "      <td>0.014305</td>\n",
              "      <td>0.020606</td>\n",
              "      <td>0.012148</td>\n",
              "      <td>0.011889</td>\n",
              "      <td>0.020353</td>\n",
              "      <td>0.010436</td>\n",
              "      <td>0.005342</td>\n",
              "      <td>0.015073</td>\n",
              "      <td>0.007208</td>\n",
              "      <td>0.017555</td>\n",
              "      <td>0.029553</td>\n",
              "      <td>0.043822</td>\n",
              "      <td>0.013829</td>\n",
              "      <td>0.018547</td>\n",
              "      <td>0.027601</td>\n",
              "      <td>0.033380</td>\n",
              "      <td>0.015125</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>0.008481</td>\n",
              "      <td>0.006676</td>\n",
              "      <td>0.038221</td>\n",
              "      <td>0.031353</td>\n",
              "      <td>0.016741</td>\n",
              "      <td>0.036852</td>\n",
              "      <td>0.024032</td>\n",
              "      <td>0.037439</td>\n",
              "      <td>0.007361</td>\n",
              "      <td>0.016260</td>\n",
              "      <td>0.014375</td>\n",
              "      <td>0.037457</td>\n",
              "      <td>0.030835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.467204</td>\n",
              "      <td>0.268642</td>\n",
              "      <td>0.130392</td>\n",
              "      <td>0.221459</td>\n",
              "      <td>0.662869</td>\n",
              "      <td>0.174224</td>\n",
              "      <td>0.366675</td>\n",
              "      <td>0.321188</td>\n",
              "      <td>0.380039</td>\n",
              "      <td>0.138031</td>\n",
              "      <td>0.064753</td>\n",
              "      <td>0.078884</td>\n",
              "      <td>0.158063</td>\n",
              "      <td>0.187518</td>\n",
              "      <td>0.246028</td>\n",
              "      <td>0.153735</td>\n",
              "      <td>0.278369</td>\n",
              "      <td>0.117534</td>\n",
              "      <td>0.197549</td>\n",
              "      <td>0.477500</td>\n",
              "      <td>0.163742</td>\n",
              "      <td>0.354168</td>\n",
              "      <td>0.194013</td>\n",
              "      <td>0.181632</td>\n",
              "      <td>0.082914</td>\n",
              "      <td>0.382064</td>\n",
              "      <td>0.158923</td>\n",
              "      <td>0.076477</td>\n",
              "      <td>0.359082</td>\n",
              "      <td>0.328868</td>\n",
              "      <td>0.161573</td>\n",
              "      <td>0.211758</td>\n",
              "      <td>0.183804</td>\n",
              "      <td>0.225270</td>\n",
              "      <td>0.131161</td>\n",
              "      <td>0.181306</td>\n",
              "      <td>0.641625</td>\n",
              "      <td>0.234535</td>\n",
              "      <td>0.339622</td>\n",
              "      <td>0.195863</td>\n",
              "      <td>...</td>\n",
              "      <td>0.250010</td>\n",
              "      <td>0.406030</td>\n",
              "      <td>0.288199</td>\n",
              "      <td>0.188608</td>\n",
              "      <td>0.446282</td>\n",
              "      <td>0.243214</td>\n",
              "      <td>0.419993</td>\n",
              "      <td>0.185247</td>\n",
              "      <td>0.073408</td>\n",
              "      <td>0.242869</td>\n",
              "      <td>0.289780</td>\n",
              "      <td>0.553350</td>\n",
              "      <td>0.473819</td>\n",
              "      <td>0.221754</td>\n",
              "      <td>0.075858</td>\n",
              "      <td>0.261542</td>\n",
              "      <td>0.222327</td>\n",
              "      <td>0.186152</td>\n",
              "      <td>0.323273</td>\n",
              "      <td>0.217372</td>\n",
              "      <td>0.286934</td>\n",
              "      <td>0.296764</td>\n",
              "      <td>0.211130</td>\n",
              "      <td>0.336977</td>\n",
              "      <td>0.294633</td>\n",
              "      <td>0.298504</td>\n",
              "      <td>0.060025</td>\n",
              "      <td>0.130622</td>\n",
              "      <td>0.047436</td>\n",
              "      <td>0.314228</td>\n",
              "      <td>0.321169</td>\n",
              "      <td>0.311798</td>\n",
              "      <td>0.279093</td>\n",
              "      <td>0.151946</td>\n",
              "      <td>0.304039</td>\n",
              "      <td>0.246636</td>\n",
              "      <td>0.184968</td>\n",
              "      <td>0.219405</td>\n",
              "      <td>0.298643</td>\n",
              "      <td>0.319475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.001993</td>\n",
              "      <td>0.008818</td>\n",
              "      <td>0.004769</td>\n",
              "      <td>0.011481</td>\n",
              "      <td>0.008770</td>\n",
              "      <td>0.021615</td>\n",
              "      <td>0.042929</td>\n",
              "      <td>0.017555</td>\n",
              "      <td>0.011563</td>\n",
              "      <td>0.019316</td>\n",
              "      <td>0.004404</td>\n",
              "      <td>0.008890</td>\n",
              "      <td>0.008754</td>\n",
              "      <td>0.035745</td>\n",
              "      <td>0.018990</td>\n",
              "      <td>0.025691</td>\n",
              "      <td>0.015542</td>\n",
              "      <td>0.026875</td>\n",
              "      <td>0.019010</td>\n",
              "      <td>0.025224</td>\n",
              "      <td>0.009780</td>\n",
              "      <td>0.025220</td>\n",
              "      <td>0.020309</td>\n",
              "      <td>0.012340</td>\n",
              "      <td>0.014860</td>\n",
              "      <td>0.080234</td>\n",
              "      <td>0.012693</td>\n",
              "      <td>0.009082</td>\n",
              "      <td>0.038183</td>\n",
              "      <td>0.014410</td>\n",
              "      <td>0.006542</td>\n",
              "      <td>0.008537</td>\n",
              "      <td>0.022572</td>\n",
              "      <td>0.017589</td>\n",
              "      <td>0.022860</td>\n",
              "      <td>0.009598</td>\n",
              "      <td>0.038316</td>\n",
              "      <td>0.027836</td>\n",
              "      <td>0.016274</td>\n",
              "      <td>0.014387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010296</td>\n",
              "      <td>0.021315</td>\n",
              "      <td>0.012014</td>\n",
              "      <td>0.017732</td>\n",
              "      <td>0.005233</td>\n",
              "      <td>0.020007</td>\n",
              "      <td>0.044639</td>\n",
              "      <td>0.021422</td>\n",
              "      <td>0.013728</td>\n",
              "      <td>0.016543</td>\n",
              "      <td>0.020551</td>\n",
              "      <td>0.006580</td>\n",
              "      <td>0.012954</td>\n",
              "      <td>0.006984</td>\n",
              "      <td>0.007922</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>0.030611</td>\n",
              "      <td>0.009226</td>\n",
              "      <td>0.025265</td>\n",
              "      <td>0.023639</td>\n",
              "      <td>0.025971</td>\n",
              "      <td>0.014458</td>\n",
              "      <td>0.013246</td>\n",
              "      <td>0.024719</td>\n",
              "      <td>0.043114</td>\n",
              "      <td>0.011896</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.012024</td>\n",
              "      <td>0.005625</td>\n",
              "      <td>0.041229</td>\n",
              "      <td>0.028435</td>\n",
              "      <td>0.009715</td>\n",
              "      <td>0.024905</td>\n",
              "      <td>0.020297</td>\n",
              "      <td>0.017200</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>0.019440</td>\n",
              "      <td>0.013148</td>\n",
              "      <td>0.039222</td>\n",
              "      <td>0.020490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.066553</td>\n",
              "      <td>0.057493</td>\n",
              "      <td>0.082297</td>\n",
              "      <td>0.027466</td>\n",
              "      <td>0.085489</td>\n",
              "      <td>0.114019</td>\n",
              "      <td>0.117804</td>\n",
              "      <td>0.093037</td>\n",
              "      <td>0.048445</td>\n",
              "      <td>0.137219</td>\n",
              "      <td>0.042447</td>\n",
              "      <td>0.027276</td>\n",
              "      <td>0.059036</td>\n",
              "      <td>0.088614</td>\n",
              "      <td>0.118063</td>\n",
              "      <td>0.098971</td>\n",
              "      <td>0.087282</td>\n",
              "      <td>0.076192</td>\n",
              "      <td>0.039715</td>\n",
              "      <td>0.060116</td>\n",
              "      <td>0.060552</td>\n",
              "      <td>0.065249</td>\n",
              "      <td>0.066101</td>\n",
              "      <td>0.074695</td>\n",
              "      <td>0.055073</td>\n",
              "      <td>0.142449</td>\n",
              "      <td>0.047002</td>\n",
              "      <td>0.052213</td>\n",
              "      <td>0.162544</td>\n",
              "      <td>0.048822</td>\n",
              "      <td>0.057765</td>\n",
              "      <td>0.061788</td>\n",
              "      <td>0.076252</td>\n",
              "      <td>0.129834</td>\n",
              "      <td>0.108836</td>\n",
              "      <td>0.095637</td>\n",
              "      <td>0.042112</td>\n",
              "      <td>0.060619</td>\n",
              "      <td>0.088194</td>\n",
              "      <td>0.059039</td>\n",
              "      <td>...</td>\n",
              "      <td>0.148272</td>\n",
              "      <td>0.140164</td>\n",
              "      <td>0.061259</td>\n",
              "      <td>0.151654</td>\n",
              "      <td>0.093488</td>\n",
              "      <td>0.053849</td>\n",
              "      <td>0.144793</td>\n",
              "      <td>0.129326</td>\n",
              "      <td>0.038617</td>\n",
              "      <td>0.073177</td>\n",
              "      <td>0.044425</td>\n",
              "      <td>0.021991</td>\n",
              "      <td>0.053031</td>\n",
              "      <td>0.093348</td>\n",
              "      <td>0.128139</td>\n",
              "      <td>0.036508</td>\n",
              "      <td>0.078303</td>\n",
              "      <td>0.019413</td>\n",
              "      <td>0.107766</td>\n",
              "      <td>0.080120</td>\n",
              "      <td>0.048282</td>\n",
              "      <td>0.094239</td>\n",
              "      <td>0.041845</td>\n",
              "      <td>0.087002</td>\n",
              "      <td>0.161701</td>\n",
              "      <td>0.047639</td>\n",
              "      <td>0.076460</td>\n",
              "      <td>0.124201</td>\n",
              "      <td>0.049763</td>\n",
              "      <td>0.060585</td>\n",
              "      <td>0.096182</td>\n",
              "      <td>0.028244</td>\n",
              "      <td>0.152077</td>\n",
              "      <td>0.066801</td>\n",
              "      <td>0.069295</td>\n",
              "      <td>0.060278</td>\n",
              "      <td>0.191969</td>\n",
              "      <td>0.028837</td>\n",
              "      <td>0.063664</td>\n",
              "      <td>0.094514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010984</td>\n",
              "      <td>0.026194</td>\n",
              "      <td>0.016028</td>\n",
              "      <td>0.028744</td>\n",
              "      <td>0.017275</td>\n",
              "      <td>0.064701</td>\n",
              "      <td>0.026352</td>\n",
              "      <td>0.045628</td>\n",
              "      <td>0.022945</td>\n",
              "      <td>0.046043</td>\n",
              "      <td>0.032279</td>\n",
              "      <td>0.060036</td>\n",
              "      <td>0.021752</td>\n",
              "      <td>0.067665</td>\n",
              "      <td>0.076109</td>\n",
              "      <td>0.041323</td>\n",
              "      <td>0.054668</td>\n",
              "      <td>0.045451</td>\n",
              "      <td>0.081171</td>\n",
              "      <td>0.057877</td>\n",
              "      <td>0.048115</td>\n",
              "      <td>0.042059</td>\n",
              "      <td>0.025430</td>\n",
              "      <td>0.015077</td>\n",
              "      <td>0.041706</td>\n",
              "      <td>0.101688</td>\n",
              "      <td>0.040118</td>\n",
              "      <td>0.027942</td>\n",
              "      <td>0.040227</td>\n",
              "      <td>0.045827</td>\n",
              "      <td>0.045220</td>\n",
              "      <td>0.028805</td>\n",
              "      <td>0.046912</td>\n",
              "      <td>0.028432</td>\n",
              "      <td>0.025471</td>\n",
              "      <td>0.021843</td>\n",
              "      <td>0.094209</td>\n",
              "      <td>0.041455</td>\n",
              "      <td>0.091371</td>\n",
              "      <td>0.042928</td>\n",
              "      <td>...</td>\n",
              "      <td>0.029420</td>\n",
              "      <td>0.033063</td>\n",
              "      <td>0.018725</td>\n",
              "      <td>0.050115</td>\n",
              "      <td>0.017315</td>\n",
              "      <td>0.040963</td>\n",
              "      <td>0.047842</td>\n",
              "      <td>0.027924</td>\n",
              "      <td>0.053845</td>\n",
              "      <td>0.032939</td>\n",
              "      <td>0.027949</td>\n",
              "      <td>0.017926</td>\n",
              "      <td>0.029639</td>\n",
              "      <td>0.031601</td>\n",
              "      <td>0.009536</td>\n",
              "      <td>0.031420</td>\n",
              "      <td>0.030474</td>\n",
              "      <td>0.031170</td>\n",
              "      <td>0.046687</td>\n",
              "      <td>0.030200</td>\n",
              "      <td>0.059149</td>\n",
              "      <td>0.044720</td>\n",
              "      <td>0.044177</td>\n",
              "      <td>0.026469</td>\n",
              "      <td>0.076348</td>\n",
              "      <td>0.023908</td>\n",
              "      <td>0.035031</td>\n",
              "      <td>0.013468</td>\n",
              "      <td>0.018153</td>\n",
              "      <td>0.074423</td>\n",
              "      <td>0.056131</td>\n",
              "      <td>0.038564</td>\n",
              "      <td>0.035467</td>\n",
              "      <td>0.044781</td>\n",
              "      <td>0.037750</td>\n",
              "      <td>0.056847</td>\n",
              "      <td>0.028048</td>\n",
              "      <td>0.012804</td>\n",
              "      <td>0.066408</td>\n",
              "      <td>0.028097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>0.821526</td>\n",
              "      <td>0.748892</td>\n",
              "      <td>0.263879</td>\n",
              "      <td>0.567762</td>\n",
              "      <td>0.441581</td>\n",
              "      <td>0.248258</td>\n",
              "      <td>0.459798</td>\n",
              "      <td>0.341347</td>\n",
              "      <td>0.606423</td>\n",
              "      <td>0.560663</td>\n",
              "      <td>0.254140</td>\n",
              "      <td>0.425927</td>\n",
              "      <td>0.299246</td>\n",
              "      <td>0.425821</td>\n",
              "      <td>0.495684</td>\n",
              "      <td>0.270094</td>\n",
              "      <td>0.437592</td>\n",
              "      <td>0.673532</td>\n",
              "      <td>0.410513</td>\n",
              "      <td>0.503604</td>\n",
              "      <td>0.511967</td>\n",
              "      <td>0.566372</td>\n",
              "      <td>0.505915</td>\n",
              "      <td>0.712008</td>\n",
              "      <td>0.135653</td>\n",
              "      <td>0.561412</td>\n",
              "      <td>0.571486</td>\n",
              "      <td>0.229754</td>\n",
              "      <td>0.671163</td>\n",
              "      <td>0.651062</td>\n",
              "      <td>0.339819</td>\n",
              "      <td>0.800328</td>\n",
              "      <td>0.654861</td>\n",
              "      <td>0.146311</td>\n",
              "      <td>0.554936</td>\n",
              "      <td>0.530773</td>\n",
              "      <td>0.277900</td>\n",
              "      <td>0.351634</td>\n",
              "      <td>0.428483</td>\n",
              "      <td>0.605215</td>\n",
              "      <td>...</td>\n",
              "      <td>0.497267</td>\n",
              "      <td>0.460793</td>\n",
              "      <td>0.492629</td>\n",
              "      <td>0.738065</td>\n",
              "      <td>0.228657</td>\n",
              "      <td>0.817378</td>\n",
              "      <td>0.512362</td>\n",
              "      <td>0.566042</td>\n",
              "      <td>0.574574</td>\n",
              "      <td>0.166190</td>\n",
              "      <td>0.726676</td>\n",
              "      <td>0.246672</td>\n",
              "      <td>0.331245</td>\n",
              "      <td>0.448688</td>\n",
              "      <td>0.219630</td>\n",
              "      <td>0.235359</td>\n",
              "      <td>0.473337</td>\n",
              "      <td>0.725732</td>\n",
              "      <td>0.582289</td>\n",
              "      <td>0.575061</td>\n",
              "      <td>0.359858</td>\n",
              "      <td>0.344130</td>\n",
              "      <td>0.306053</td>\n",
              "      <td>0.332169</td>\n",
              "      <td>0.527686</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.765765</td>\n",
              "      <td>0.497892</td>\n",
              "      <td>0.340756</td>\n",
              "      <td>0.304399</td>\n",
              "      <td>0.373491</td>\n",
              "      <td>0.532200</td>\n",
              "      <td>0.767381</td>\n",
              "      <td>0.613560</td>\n",
              "      <td>0.447047</td>\n",
              "      <td>0.361896</td>\n",
              "      <td>0.725515</td>\n",
              "      <td>0.500748</td>\n",
              "      <td>0.227037</td>\n",
              "      <td>0.474251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>0.002295</td>\n",
              "      <td>0.010723</td>\n",
              "      <td>0.005563</td>\n",
              "      <td>0.014165</td>\n",
              "      <td>0.005721</td>\n",
              "      <td>0.024850</td>\n",
              "      <td>0.019220</td>\n",
              "      <td>0.014036</td>\n",
              "      <td>0.014727</td>\n",
              "      <td>0.016570</td>\n",
              "      <td>0.005150</td>\n",
              "      <td>0.009599</td>\n",
              "      <td>0.009693</td>\n",
              "      <td>0.034690</td>\n",
              "      <td>0.019426</td>\n",
              "      <td>0.024931</td>\n",
              "      <td>0.015997</td>\n",
              "      <td>0.026350</td>\n",
              "      <td>0.021024</td>\n",
              "      <td>0.023593</td>\n",
              "      <td>0.008915</td>\n",
              "      <td>0.035345</td>\n",
              "      <td>0.021297</td>\n",
              "      <td>0.009872</td>\n",
              "      <td>0.015551</td>\n",
              "      <td>0.044737</td>\n",
              "      <td>0.009059</td>\n",
              "      <td>0.006239</td>\n",
              "      <td>0.025183</td>\n",
              "      <td>0.017722</td>\n",
              "      <td>0.007656</td>\n",
              "      <td>0.010190</td>\n",
              "      <td>0.022579</td>\n",
              "      <td>0.007150</td>\n",
              "      <td>0.013729</td>\n",
              "      <td>0.010883</td>\n",
              "      <td>0.030102</td>\n",
              "      <td>0.024461</td>\n",
              "      <td>0.016519</td>\n",
              "      <td>0.013035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011546</td>\n",
              "      <td>0.020734</td>\n",
              "      <td>0.012194</td>\n",
              "      <td>0.016188</td>\n",
              "      <td>0.005915</td>\n",
              "      <td>0.017878</td>\n",
              "      <td>0.026694</td>\n",
              "      <td>0.020450</td>\n",
              "      <td>0.011387</td>\n",
              "      <td>0.014251</td>\n",
              "      <td>0.020608</td>\n",
              "      <td>0.009070</td>\n",
              "      <td>0.011702</td>\n",
              "      <td>0.008443</td>\n",
              "      <td>0.004045</td>\n",
              "      <td>0.006300</td>\n",
              "      <td>0.009443</td>\n",
              "      <td>0.007501</td>\n",
              "      <td>0.013077</td>\n",
              "      <td>0.021442</td>\n",
              "      <td>0.031758</td>\n",
              "      <td>0.024298</td>\n",
              "      <td>0.013187</td>\n",
              "      <td>0.024827</td>\n",
              "      <td>0.033180</td>\n",
              "      <td>0.015517</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>0.006186</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>0.030715</td>\n",
              "      <td>0.029776</td>\n",
              "      <td>0.011739</td>\n",
              "      <td>0.027360</td>\n",
              "      <td>0.023094</td>\n",
              "      <td>0.017549</td>\n",
              "      <td>0.009484</td>\n",
              "      <td>0.013856</td>\n",
              "      <td>0.014003</td>\n",
              "      <td>0.032914</td>\n",
              "      <td>0.020923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.015046</td>\n",
              "      <td>0.005295</td>\n",
              "      <td>0.013603</td>\n",
              "      <td>0.008103</td>\n",
              "      <td>0.017376</td>\n",
              "      <td>0.020909</td>\n",
              "      <td>0.014589</td>\n",
              "      <td>0.015857</td>\n",
              "      <td>0.019484</td>\n",
              "      <td>0.004788</td>\n",
              "      <td>0.012232</td>\n",
              "      <td>0.006252</td>\n",
              "      <td>0.040468</td>\n",
              "      <td>0.024855</td>\n",
              "      <td>0.026366</td>\n",
              "      <td>0.023058</td>\n",
              "      <td>0.027585</td>\n",
              "      <td>0.011470</td>\n",
              "      <td>0.026881</td>\n",
              "      <td>0.016066</td>\n",
              "      <td>0.025906</td>\n",
              "      <td>0.025160</td>\n",
              "      <td>0.014291</td>\n",
              "      <td>0.013823</td>\n",
              "      <td>0.048612</td>\n",
              "      <td>0.008994</td>\n",
              "      <td>0.010151</td>\n",
              "      <td>0.025729</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.007814</td>\n",
              "      <td>0.010359</td>\n",
              "      <td>0.028412</td>\n",
              "      <td>0.008929</td>\n",
              "      <td>0.011726</td>\n",
              "      <td>0.011115</td>\n",
              "      <td>0.038823</td>\n",
              "      <td>0.019357</td>\n",
              "      <td>0.015480</td>\n",
              "      <td>0.016701</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006021</td>\n",
              "      <td>0.019790</td>\n",
              "      <td>0.010365</td>\n",
              "      <td>0.015953</td>\n",
              "      <td>0.005901</td>\n",
              "      <td>0.019228</td>\n",
              "      <td>0.030009</td>\n",
              "      <td>0.020025</td>\n",
              "      <td>0.011848</td>\n",
              "      <td>0.013118</td>\n",
              "      <td>0.017047</td>\n",
              "      <td>0.008968</td>\n",
              "      <td>0.011663</td>\n",
              "      <td>0.014660</td>\n",
              "      <td>0.004288</td>\n",
              "      <td>0.006419</td>\n",
              "      <td>0.011989</td>\n",
              "      <td>0.010235</td>\n",
              "      <td>0.016969</td>\n",
              "      <td>0.027182</td>\n",
              "      <td>0.024286</td>\n",
              "      <td>0.014077</td>\n",
              "      <td>0.016795</td>\n",
              "      <td>0.020484</td>\n",
              "      <td>0.031493</td>\n",
              "      <td>0.010722</td>\n",
              "      <td>0.001996</td>\n",
              "      <td>0.007573</td>\n",
              "      <td>0.006417</td>\n",
              "      <td>0.038639</td>\n",
              "      <td>0.029881</td>\n",
              "      <td>0.015148</td>\n",
              "      <td>0.030170</td>\n",
              "      <td>0.024205</td>\n",
              "      <td>0.022857</td>\n",
              "      <td>0.008639</td>\n",
              "      <td>0.012611</td>\n",
              "      <td>0.015545</td>\n",
              "      <td>0.034892</td>\n",
              "      <td>0.025188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>0.076153</td>\n",
              "      <td>0.194873</td>\n",
              "      <td>0.089162</td>\n",
              "      <td>0.052825</td>\n",
              "      <td>0.055414</td>\n",
              "      <td>0.198624</td>\n",
              "      <td>0.306829</td>\n",
              "      <td>0.115475</td>\n",
              "      <td>0.241576</td>\n",
              "      <td>0.189678</td>\n",
              "      <td>0.082151</td>\n",
              "      <td>0.063423</td>\n",
              "      <td>0.032939</td>\n",
              "      <td>0.071732</td>\n",
              "      <td>0.258989</td>\n",
              "      <td>0.109396</td>\n",
              "      <td>0.238514</td>\n",
              "      <td>0.257378</td>\n",
              "      <td>0.346965</td>\n",
              "      <td>0.107715</td>\n",
              "      <td>0.050878</td>\n",
              "      <td>0.041961</td>\n",
              "      <td>0.267622</td>\n",
              "      <td>0.160759</td>\n",
              "      <td>0.443613</td>\n",
              "      <td>0.129309</td>\n",
              "      <td>0.103893</td>\n",
              "      <td>0.056711</td>\n",
              "      <td>0.120178</td>\n",
              "      <td>0.081161</td>\n",
              "      <td>0.049486</td>\n",
              "      <td>0.213296</td>\n",
              "      <td>0.062874</td>\n",
              "      <td>0.051067</td>\n",
              "      <td>0.399944</td>\n",
              "      <td>0.199866</td>\n",
              "      <td>0.207089</td>\n",
              "      <td>0.097512</td>\n",
              "      <td>0.062044</td>\n",
              "      <td>0.112497</td>\n",
              "      <td>...</td>\n",
              "      <td>0.289886</td>\n",
              "      <td>0.079692</td>\n",
              "      <td>0.213936</td>\n",
              "      <td>0.207623</td>\n",
              "      <td>0.022039</td>\n",
              "      <td>0.244028</td>\n",
              "      <td>0.326713</td>\n",
              "      <td>0.233672</td>\n",
              "      <td>0.194297</td>\n",
              "      <td>0.105329</td>\n",
              "      <td>0.260498</td>\n",
              "      <td>0.118653</td>\n",
              "      <td>0.094601</td>\n",
              "      <td>0.043866</td>\n",
              "      <td>0.067808</td>\n",
              "      <td>0.120663</td>\n",
              "      <td>0.091215</td>\n",
              "      <td>0.132479</td>\n",
              "      <td>0.199108</td>\n",
              "      <td>0.103690</td>\n",
              "      <td>0.133020</td>\n",
              "      <td>0.248928</td>\n",
              "      <td>0.059431</td>\n",
              "      <td>0.045112</td>\n",
              "      <td>0.203306</td>\n",
              "      <td>0.143942</td>\n",
              "      <td>0.049856</td>\n",
              "      <td>0.176028</td>\n",
              "      <td>0.026570</td>\n",
              "      <td>0.208772</td>\n",
              "      <td>0.205970</td>\n",
              "      <td>0.194160</td>\n",
              "      <td>0.330962</td>\n",
              "      <td>0.136074</td>\n",
              "      <td>0.076991</td>\n",
              "      <td>0.040400</td>\n",
              "      <td>0.334105</td>\n",
              "      <td>0.041088</td>\n",
              "      <td>0.138143</td>\n",
              "      <td>0.137474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>0.572953</td>\n",
              "      <td>0.504904</td>\n",
              "      <td>0.167938</td>\n",
              "      <td>0.310852</td>\n",
              "      <td>0.453982</td>\n",
              "      <td>0.516723</td>\n",
              "      <td>0.334641</td>\n",
              "      <td>0.628016</td>\n",
              "      <td>0.398844</td>\n",
              "      <td>0.180618</td>\n",
              "      <td>0.234518</td>\n",
              "      <td>0.382439</td>\n",
              "      <td>0.677890</td>\n",
              "      <td>0.537229</td>\n",
              "      <td>0.508850</td>\n",
              "      <td>0.571865</td>\n",
              "      <td>0.424064</td>\n",
              "      <td>0.228936</td>\n",
              "      <td>0.224128</td>\n",
              "      <td>0.617121</td>\n",
              "      <td>0.455694</td>\n",
              "      <td>0.570267</td>\n",
              "      <td>0.587917</td>\n",
              "      <td>0.582916</td>\n",
              "      <td>0.516728</td>\n",
              "      <td>0.554465</td>\n",
              "      <td>0.766420</td>\n",
              "      <td>0.477935</td>\n",
              "      <td>0.305039</td>\n",
              "      <td>0.375715</td>\n",
              "      <td>0.700158</td>\n",
              "      <td>0.128150</td>\n",
              "      <td>0.386287</td>\n",
              "      <td>0.577858</td>\n",
              "      <td>0.364437</td>\n",
              "      <td>0.294630</td>\n",
              "      <td>0.568776</td>\n",
              "      <td>0.334003</td>\n",
              "      <td>0.553902</td>\n",
              "      <td>0.358176</td>\n",
              "      <td>...</td>\n",
              "      <td>0.258442</td>\n",
              "      <td>0.680600</td>\n",
              "      <td>0.819185</td>\n",
              "      <td>0.682229</td>\n",
              "      <td>0.245421</td>\n",
              "      <td>0.782180</td>\n",
              "      <td>0.336985</td>\n",
              "      <td>0.409072</td>\n",
              "      <td>0.320435</td>\n",
              "      <td>0.480223</td>\n",
              "      <td>0.168607</td>\n",
              "      <td>0.256741</td>\n",
              "      <td>0.607945</td>\n",
              "      <td>0.280362</td>\n",
              "      <td>0.551178</td>\n",
              "      <td>0.538110</td>\n",
              "      <td>0.258717</td>\n",
              "      <td>0.188474</td>\n",
              "      <td>0.618772</td>\n",
              "      <td>0.304628</td>\n",
              "      <td>0.441413</td>\n",
              "      <td>0.575394</td>\n",
              "      <td>0.413610</td>\n",
              "      <td>0.656014</td>\n",
              "      <td>0.615486</td>\n",
              "      <td>0.687402</td>\n",
              "      <td>0.455690</td>\n",
              "      <td>0.485238</td>\n",
              "      <td>0.524384</td>\n",
              "      <td>0.587045</td>\n",
              "      <td>0.647339</td>\n",
              "      <td>0.582324</td>\n",
              "      <td>0.275835</td>\n",
              "      <td>0.904409</td>\n",
              "      <td>0.179764</td>\n",
              "      <td>0.127489</td>\n",
              "      <td>0.500809</td>\n",
              "      <td>0.573487</td>\n",
              "      <td>0.522815</td>\n",
              "      <td>0.653068</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>350 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2   ...        97        98        99\n",
              "0    0.004702  0.021260  0.008639  ...  0.014375  0.037457  0.030835\n",
              "1    0.467204  0.268642  0.130392  ...  0.219405  0.298643  0.319475\n",
              "2    0.001993  0.008818  0.004769  ...  0.013148  0.039222  0.020490\n",
              "3    0.066553  0.057493  0.082297  ...  0.028837  0.063664  0.094514\n",
              "4    0.010984  0.026194  0.016028  ...  0.012804  0.066408  0.028097\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "345  0.821526  0.748892  0.263879  ...  0.500748  0.227037  0.474251\n",
              "346  0.002295  0.010723  0.005563  ...  0.014003  0.032914  0.020923\n",
              "347  0.004100  0.015046  0.005295  ...  0.015545  0.034892  0.025188\n",
              "348  0.076153  0.194873  0.089162  ...  0.041088  0.138143  0.137474\n",
              "349  0.572953  0.504904  0.167938  ...  0.573487  0.522815  0.653068\n",
              "\n",
              "[350 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDw1AthCLdLN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "280e2e63-2d10-4c84-e7e9-46ab5b4b83bb"
      },
      "source": [
        "sum(y_sub),len(y_sub)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175, 350)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa-gkQ8FLbUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rcPvmHqJ8im",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "769b48b8-b906-4c27-e529-a972df88b1e7"
      },
      "source": [
        "files.download('submit_test.csv')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_0e4a8eed-afb2-4f43-92e8-dce7f10fd64f\", \"submit_test.csv\", 1990)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOBxVu3_K_iE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "fa2d573a-1ed8-4eae-81b0-ffde711fca29"
      },
      "source": [
        "67f6u7=ihoioo;gh@0]h"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-63-e69833c89e73>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    67f6u7=ihoioo;gh@0]h\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7nKKSVHmA-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}